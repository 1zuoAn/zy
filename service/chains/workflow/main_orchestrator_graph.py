from __future__ import annotations

import time
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Dict, List, Literal, Optional

from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage
from langgraph.graph import END, START, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.prebuilt import ToolNode
from loguru import logger
from sqlalchemy import text

from app.config import settings
from app.core.clients.db_client import pg_session
from app.core.clients.redis_client import redis_client
from app.core.config.constants import (
    DBAlias,
    LlmModelName,
    LlmProvider,
    RedisMessageKeyName,
)
from app.core.tools import llm_factory
from app.schemas.entities.message.redis_message import (
    BaseRedisMessage,
    TextMessageContent,
)
from app.schemas.entities.workflow.graph_state import MainOrchestratorState
from app.schemas.request.workflow_request import WorkflowRequest
from app.schemas.response.workflow_response import WorkflowResponse
from app.service.chains.workflow.base_graph import BaseWorkflowGraph
from app.service.chains.workflow.orchestrator_tools import ALL_TOOLS
from app.service.chains.workflow.artifact_store import get_artifact_store
from app.service.rpc.vlm_service import get_vlm_service
from app.utils import thread_pool


class MainOrchestratorGraph(BaseWorkflowGraph):
    """
    ä¸»agent - æ ‡å‡† LangGraph ReAct

    å›¾ç»“æ„ (3 èŠ‚ç‚¹):

        START â”€â”€â†’ agent â†â”€â”€â†’ tools â”€â”€â†’ postprocess â”€â”€â†’ END
                    â†‘         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   (ReAct loop)
    """

    span_name = "zxy_agent_system"
    run_name = "zxy_agent_system"
    _max_agent_iterations = 6

    def __init__(self) -> None:
        super().__init__()

    def _build_graph(self) -> CompiledStateGraph:
        """æ„å»º LangGraph æ‰§è¡Œå›¾ï¼ˆæ ‡å‡† ReAct å¾ªç¯ï¼‰"""
        graph = StateGraph(MainOrchestratorState)

        # Agent èŠ‚ç‚¹ï¼šLLM æ¨ç†ä¸å†³ç­–
        graph.add_node("agent", self._agent_node)

        # Tools èŠ‚ç‚¹ï¼šç›´æ¥ä½¿ç”¨ ToolNodeï¼ˆç¬¦åˆ LangGraph è§„èŒƒï¼ŒGraph ä¼šè‡ªåŠ¨æä¾› runtimeï¼‰
        graph.add_node("tools", ToolNode(ALL_TOOLS))

        # Postprocess èŠ‚ç‚¹ï¼šæå– artifactsã€æ¨é€ç»“æœã€ä¿å­˜å›å¤
        graph.add_node("postprocess", self._postprocess_node)

        # å®šä¹‰è¾¹
        graph.add_edge(START, "agent")  # èµ·ç‚¹

        # æ¡ä»¶è¾¹ï¼šç”± _should_continue åˆ¤æ–­æ˜¯å¦ç»§ç»­è°ƒç”¨å·¥å…·
        graph.add_conditional_edges(
            "agent",
            self._should_continue,
            {"continue": "tools", "end": "postprocess"},
        )

        graph.add_edge("tools", "agent")  # ReAct å¾ªç¯ï¼šå·¥å…·æ‰§è¡Œåè¿”å› agent ç»§ç»­æ¨ç†
        graph.add_edge("postprocess", END)  # ç»ˆç‚¹

        return graph.compile()

    # ==================== Agent èŠ‚ç‚¹ ====================

    def _agent_node(self, state: MainOrchestratorState) -> Dict[str, Any]:
        """ä¸» Agent èŠ‚ç‚¹ï¼šLLM æ¨ç† + å·¥å…·è°ƒç”¨å†³ç­–ï¼ˆReAct èŒƒå¼ï¼šæ— ç¡¬ç¼–ç è§„åˆ™ï¼Œå®Œå…¨ç”± LLM è‡ªä¸»å†³ç­–ï¼‰"""
        messages = state.get("messages") or []

        if not messages:
            # é¦–æ¬¡è°ƒç”¨ï¼šæ„å»ºåˆå§‹æ¶ˆæ¯å¹¶å†™å…¥ state
            messages, upload_image_ids = self._prepare_initial_messages(state)
            ai_message = self._invoke_llm_with_fallback(messages, state)
            self._log_response(ai_message)
            return {"messages": messages + [ai_message], "upload_image_ids": upload_image_ids}

        # åç»­è°ƒç”¨ï¼šç›´æ¥ä½¿ç”¨ state ä¸­çš„æ¶ˆæ¯
        ai_message = self._invoke_llm_with_fallback(messages, state)
        self._log_response(ai_message)
        return {"messages": [ai_message]}

    def _log_response(self, ai_message: AIMessage) -> None:
        """è®°å½• LLM å“åº”æ‘˜è¦"""
        content = getattr(ai_message, "content", "")
        tool_calls = getattr(ai_message, "tool_calls", None)
        if tool_calls:
            tool_names = [tc.get("name") for tc in tool_calls]
            logger.info(f"[Agent] å·¥å…·è°ƒç”¨: {tool_names}")
        elif content:
            logger.info(f"[Agent] å›å¤: {content[:100]}...")

    def _invoke_llm_with_fallback(
        self,
        messages: List,
        state: MainOrchestratorState,
    ) -> AIMessage:
        """è°ƒç”¨ LLMï¼Œå¸¦é‡è¯•å’Œé™çº§ç­–ç•¥"""
        try:
            # ä½¿ç”¨ Kimi K2 Thinkingï¼ˆæ”¯æŒ extended thinkingï¼‰
            from app.core.tools.llm_factory import LLMFactory
            llm: BaseChatModel = LLMFactory.create_openrouter_llm(
                model="moonshotai/kimi-k2-thinking",
                max_tokens=16384,
            )
        except Exception as e:
            logger.warning(f"[Agent] LLM initialization failed: {e}")
            return self._fallback_response(state, e)

        # è°ƒç”¨ LLMï¼Œå¸¦æŒ‡æ•°é€€é¿é‡è¯•ï¼ˆæœ€å¤š 2 æ¬¡ï¼‰
        ai_message = self._invoke_with_retry(llm, messages, max_retries=2)
        if ai_message:
            return ai_message

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ â†’ é™çº§ï¼ˆå‹å¥½é”™è¯¯æç¤ºï¼Œä¸ç”¨ç¡¬ç¼–ç è§„åˆ™æ›¿ä»£ï¼‰
        return self._fallback_response(state, Exception("LLM retry exhausted"))

    def _fallback_response(
        self,
        state: MainOrchestratorState,
        error: Exception | None = None,
    ) -> AIMessage:
        """é™çº§å“åº”ï¼šä»…åœ¨ LLM å®Œå…¨å¤±è´¥æ—¶ä½¿ç”¨ï¼ˆåªåšå‹å¥½é”™è¯¯å¤„ç†ï¼Œä¸ç”¨è§„åˆ™æ›¿ä»£ LLM å†³ç­–ï¼‰"""
        if error:
            logger.error(f"[Fallback] LLM failed: {error}")

        # ç¬¦åˆ ReAct èŒƒå¼çš„é™çº§ç­–ç•¥ï¼šè¯šå®å‘ŠçŸ¥ç”¨æˆ·ï¼Œè€Œä¸æ˜¯ç”¨è§„åˆ™ç³»ç»Ÿä¼ªè£…æˆæ™ºèƒ½å†³ç­–
        return AIMessage(
            content="æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼Œæ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚ã€‚è¯·ç¨åå†è¯•ï¼Œæˆ–è€…æ¢ä¸€ç§æ–¹å¼æè¿°æ‚¨çš„éœ€æ±‚ã€‚"
        )

    def _invoke_with_retry(
        self, llm: BaseChatModel, messages: List, max_retries: int = 2
    ) -> Optional[AIMessage]:
        """å¸¦æŒ‡æ•°é€€é¿å’Œé”™è¯¯åˆ†ç±»çš„ LLM è°ƒç”¨"""
        for attempt in range(max_retries):
            try:
                # bind_tools è®© LLM çŸ¥é“å¯ç”¨çš„å·¥å…·åŠå…¶å‚æ•°ï¼ˆLLM è‡ªä¸»å†³å®šè°ƒç”¨å“ªäº›ï¼Œæ— ç¡¬ç¼–ç è§„åˆ™ï¼‰
                result = llm.bind_tools(ALL_TOOLS).invoke(messages)
                if not isinstance(result, AIMessage):
                    result = AIMessage(content=str(getattr(result, "content", result)))
                return result
            except Exception as e:
                error_msg = str(e).lower()

                if "rate" in error_msg or "limit" in error_msg or "429" in error_msg:
                    # é™æµé”™è¯¯ï¼šæŒ‡æ•°é€€é¿ï¼ˆ1s, 2sï¼‰
                    logger.warning(f"[Agent] é™æµé”™è¯¯(å°è¯• {attempt + 1}): {e}")
                    wait_time = (2**attempt) * 1
                    time.sleep(wait_time)
                elif "content" in error_msg and "filter" in error_msg:
                    # å†…å®¹å®¡æ ¸é”™è¯¯ï¼šç«‹å³è¿”å›ï¼Œä¸é‡è¯•
                    logger.warning(f"[Agent] å†…å®¹å®¡æ ¸æ‹’ç»: {e}")
                    return AIMessage(content="æŠ±æ­‰ï¼Œæ‚¨çš„è¯·æ±‚åŒ…å«ä¸æ”¯æŒçš„å†…å®¹ï¼Œè¯·è°ƒæ•´åé‡è¯•ã€‚")
                else:
                    # å…¶ä»–é”™è¯¯ï¼šçº¿æ€§ç­‰å¾… 1s åé‡è¯•
                    logger.warning(f"[Agent] å…¶ä»–é”™è¯¯(å°è¯• {attempt + 1}): {e}")
                    if attempt < max_retries - 1:
                        time.sleep(1)
        return None

    def _prepare_initial_messages(self, state: MainOrchestratorState) -> tuple[List, List[str]]:
        """å‡†å¤‡åˆå§‹æ¶ˆæ¯ï¼ˆé¦–æ¬¡è¿›å…¥ agent æ—¶è°ƒç”¨ï¼‰"""
        req: WorkflowRequest = state["request"]

        # å¼‚æ­¥è®°å½•ç”¨æˆ·æŸ¥è¯¢åˆ°æ•°æ®åº“ï¼ˆä¸é˜»å¡ä¸»æµç¨‹ï¼‰
        self._record_user_query(req)

        # å¤„ç†ä¸Šä¼ å›¾ç‰‡ï¼šå¹¶è¡Œè°ƒç”¨ VLM â†’ åˆ›å»º Artifact â†’ ç”Ÿæˆæè¿°æ–‡æœ¬
        # âš ï¸ å”¯ä¸€è°ƒç”¨ VLM çš„åœ°æ–¹ï¼Œåç»­é€šè¿‡ inspect_artifact ç›´æ¥è¿”å› content_cache
        upload_image_ids, image_content = self._extract_images_as_artifacts(req)

        # æŸ¥è¯¢ä¼šè¯å†å²ï¼ˆé™åˆ¶æœ€è¿‘ 20 æ¡ï¼Œé¿å… context è¿‡é•¿ï¼‰
        history_messages = self._query_history(req)

        # æ„å»ºå®Œæ•´æ¶ˆæ¯åˆ—è¡¨ï¼ˆSystem Prompt + History + Currentï¼‰
        messages = self._build_messages(req, image_content, history_messages, upload_image_ids)
        return messages, upload_image_ids

    def _record_user_query(self, req: WorkflowRequest) -> None:
        """è®°å½•ç”¨æˆ·æŸ¥è¯¢åˆ°æ•°æ®åº“"""

        def insert_track() -> None:
            try:
                with pg_session(DBAlias.DB_ABROAD_AI.value) as session:
                    session.execute(
                        text(
                            """
                            INSERT INTO n8n_user_query_message(session_id, user_query)
                            VALUES (:session_id, :user_query)
                        """
                        ),
                        {"session_id": req.session_id, "user_query": f"human:{req.user_query}"},
                    )
            except Exception as e:
                logger.error(f"[ä¸»å·¥ä½œæµ] è®°å½•ç”¨æˆ·æŸ¥è¯¢å¤±è´¥: {e}")

        thread_pool.submit_with_context(insert_track)

    def _extract_images_as_artifacts(self, req: WorkflowRequest) -> tuple[List[str], Optional[str]]:
        """æå–ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡ï¼Œå¹¶è¡Œè°ƒç”¨ VLM ç”Ÿæˆæè¿°ï¼Œåˆ›å»º Artifactï¼ˆâš ï¸ VLM å”¯ä¸€è°ƒç”¨ç‚¹ï¼‰"""
        images = getattr(req, "images", None) or []
        if not images:
            return [], None

        artifact_store = get_artifact_store()
        artifacts: dict[int, str] = {}  # idx -> artifact_id
        captions: dict[int, str] = {}   # idx -> caption

        max_workers = max(1, min(len(images), settings.main_workflow_image_max_workers))

        # å¹¶è¡Œå¤„ç†å›¾ç‰‡ï¼ˆVLM è°ƒç”¨è€—æ—¶ï¼Œå¿…é¡»å¹¶è¡Œï¼‰
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_index = {
                executor.submit(self._extract_single_image, url): idx
                for idx, url in enumerate(images)
            }
            for future in as_completed(future_to_index):
                idx = future_to_index[future]
                caption = future.result()
                if caption:
                    artifact_id = f"img_upload_{int(time.time() * 1000)}_{idx}"
                    record = {
                        "id": artifact_id,
                        "type": "image_asset",
                        "description": f"ç”¨æˆ·ä¸Šä¼ çš„å›¾ç‰‡ {idx + 1}",
                        "content_cache": caption,
                        "meta": {"source": "user_upload", "index": idx + 1, "url": images[idx]},
                    }
                    artifact_store.save_payload(req.session_id, artifact_id, record)
                    artifacts[idx] = artifact_id
                    captions[idx] = caption

        # ç”Ÿæˆæ ¼å¼åŒ–æè¿°æ–‡æœ¬ï¼ˆæŒ‰åŸå§‹ç´¢å¼•æ’åºï¼Œç¡®ä¿ artifact_id ä¸ caption æ­£ç¡®å¯¹åº”ï¼‰
        if captions:
            sorted_indices = sorted(captions.keys())
            lines = [
                f"[Image {i + 1}] (artifact: {artifacts[i]}): {captions[i]}"
                for i in sorted_indices
            ]
            artifact_ids = [artifacts[i] for i in sorted_indices]
            return artifact_ids, "\n".join(lines)

        return [], None

    def _extract_single_image(self, image_url: str) -> Optional[str]:
        """æå–å•å¼ å›¾ç‰‡çš„æè¿°ï¼ˆä½¿ç”¨ VLMServiceï¼‰"""
        max_retries = settings.main_workflow_image_retry_attempts
        vlm_service = get_vlm_service()

        for attempt in range(max_retries):
            try:
                caption = vlm_service.describe(image_url)
                return caption
            except Exception as e:
                logger.warning(f"[å›¾ç‰‡æå–] ç¬¬{attempt + 1}æ¬¡å°è¯•å¤±è´¥: {e}")
                if attempt == max_retries - 1:
                    # é™çº§ï¼šè¿”å›é»˜è®¤æè¿°
                    return "ç”¨æˆ·ä¸Šä¼ çš„å›¾ç‰‡"
        return "ç”¨æˆ·ä¸Šä¼ çš„å›¾ç‰‡"

    def _query_history(self, req: WorkflowRequest, limit: int = 20) -> List:
        """æŸ¥è¯¢ä¼šè¯å†å²å¹¶è½¬æ¢ä¸º LangGraph Messagesï¼ˆé™åˆ¶æœ€è¿‘ N æ¡ï¼‰

        Args:
            req: è¯·æ±‚å¯¹è±¡
            limit: æœ€å¤šæŸ¥è¯¢å¤šå°‘æ¡å†å²è®°å½•

        Returns:
            List[HumanMessage | AIMessage]: å†å²æ¶ˆæ¯åˆ—è¡¨ï¼ŒæŒ‰æ—¶é—´æ­£åºæ’åˆ—
        """
        history_messages: List = []
        try:
            with pg_session(DBAlias.DB_ABROAD_AI.value) as session:
                result = session.execute(
                    text(
                        """
                        SELECT id, session_id, user_query
                        FROM n8n_user_query_message
                        WHERE session_id = :session_id
                        ORDER BY id DESC
                        LIMIT :limit
                    """
                    ),
                    {"session_id": req.session_id, "limit": limit},
                )
                rows = list(result.mappings().all())

                # åè½¬é¡ºåºï¼Œä¿æŒæ—¶é—´æ­£åº
                for row in reversed(rows):
                    query_text = row["user_query"]

                    # æ ¹æ®å‰ç¼€åˆ¤æ–­æ¶ˆæ¯ç±»å‹
                    if query_text.startswith("human:"):
                        content = query_text[6:].strip()  # å»æ‰ "human:" å‰ç¼€
                        history_messages.append(HumanMessage(content=content))
                    elif query_text.startswith("ai:"):
                        content = query_text[3:].strip()  # å»æ‰ "ai:" å‰ç¼€
                        history_messages.append(AIMessage(content=content))
                    else:
                        # å…¼å®¹æ—§æ•°æ®ï¼šæ— å‰ç¼€æ—¶é»˜è®¤ä¸ºç”¨æˆ·æ¶ˆæ¯
                        logger.warning(f"[ä¼šè¯å†å²] æ¶ˆæ¯ {row['id']} ç¼ºå°‘å‰ç¼€ï¼Œé»˜è®¤ä¸ºç”¨æˆ·æ¶ˆæ¯")
                        history_messages.append(HumanMessage(content=query_text))

                logger.info(f"[ä¼šè¯å†å²] åŠ è½½äº† {len(history_messages)} æ¡å†å²æ¶ˆæ¯")

        except Exception as e:
            logger.warning(f"[ä¼šè¯å†å²] æŸ¥è¯¢å¤±è´¥: {e}")

        return history_messages

    def _build_messages(
        self,
        req: WorkflowRequest,
        image_content: Optional[str],
        history_messages: List,
        upload_image_ids: Optional[List[str]] = None,
    ) -> List:
        """æ„å»ºåˆå§‹æ¶ˆæ¯ï¼šSystem + History + Current

        Args:
            req: è¯·æ±‚å¯¹è±¡
            image_content: å›¾ç‰‡æè¿°æ–‡æœ¬
            history_messages: å†å²æ¶ˆæ¯åˆ—è¡¨ï¼ˆHumanMessage/AIMessageï¼‰
            upload_image_ids: ä¸Šä¼ å›¾ç‰‡çš„ artifact IDs

        Returns:
            List: å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨
        """
        tool_names = ", ".join([tool.name for tool in ALL_TOOLS])

        system_prompt = f"""ä½ æ˜¯çŸ¥å°è¡£ï¼Œä¸€ä¸ªç”±çŸ¥è¡£ç§‘æŠ€å¼€å‘çš„æ™ºèƒ½æœè£…é€‰å“ä¸è®¾è®¡åŠ©æ‰‹ã€‚

## æ ¸å¿ƒèƒ½åŠ›

1. **å•†å“æœç´¢**ï¼šæ·˜å®ï¼ˆselect_zhiyiï¼‰ã€æŠ–éŸ³ï¼ˆselect_douyiï¼‰ã€æµ·å¤–å¹³å°
2. **è®¾è®¡ç”Ÿæˆ**ï¼šAI ç”Ÿå›¾ï¼ˆcreate_imageï¼‰ã€AI æ”¹å›¾ï¼ˆedit_imageï¼‰
3. **å†…å®¹æœç´¢**ï¼šINS åšä¸»ã€å°çº¢ä¹¦å†…å®¹
4. **ä»»åŠ¡ç®¡ç†**ï¼šå®šæ—¶ç›‘æ§ä»»åŠ¡

## å·¥ä½œæµç¨‹

1. **ç†è§£éœ€æ±‚**ï¼šåˆ†æç”¨æˆ·æƒ³è¦ä»€ä¹ˆï¼Œè¯†åˆ«å…³é”®ä¿¡æ¯ï¼ˆå¹³å°ã€ç±»ç›®ã€é£æ ¼ç­‰ï¼‰
2. **é€‰æ‹©å·¥å…·**ï¼šæ ¹æ®éœ€æ±‚å†³å®šè°ƒç”¨å“ªäº›å·¥å…·ï¼Œå¯å¹¶è¡Œè°ƒç”¨å¤šä¸ª
3. **æ‰§è¡Œå¹¶è¿”å›**ï¼šè°ƒç”¨å·¥å…·è·å–ç»“æœï¼Œç»™å‡ºç®€æ´æ¸…æ™°çš„å›å¤

## èµ„äº§ç³»ç»Ÿï¼ˆå†…éƒ¨ä½¿ç”¨ï¼Œä¸è¦å‘ç”¨æˆ·å±•ç¤ºï¼‰

å·¥å…·è¿”å›çš„æ•°æ®ä»¥ã€Œèµ„äº§ã€å½¢å¼å­˜å‚¨ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è®°å½•ã€‚
- ç”¨æˆ·æåˆ°"ä¸Šä¸€è½®"/"åˆšæ‰"çš„ç»“æœæ—¶ï¼Œä»å†å²æ¶ˆæ¯ä¸­æ‰¾åˆ°å¯¹åº”çš„ artifact_id
- éœ€è¦æŸ¥çœ‹èµ„äº§è¯¦æƒ…æ—¶ï¼Œè°ƒç”¨ `inspect_artifact(artifact_id)`
- âš ï¸ ä¸è¦å‘ç”¨æˆ·å±•ç¤º artifact_idï¼Œç”¨æˆ·åªéœ€è¦çŸ¥é“"æ‰¾åˆ°äº†å¤šå°‘ä»¶"

## å¯ç”¨å·¥å…·

{tool_names}

## å›å¤è¦æ±‚

1. åªè¾“å‡ºç»™ç”¨æˆ·çœ‹çš„å†…å®¹ï¼Œä¸è¦è¾“å‡ºå†…éƒ¨æ€è€ƒè¿‡ç¨‹
2. ä¸è¦å±•ç¤º artifact_id ç»™ç”¨æˆ·
3. å›å¤ç®€æ´è‡ªç„¶ï¼Œåƒæ­£å¸¸å¯¹è¯ä¸€æ ·

ç¤ºä¾‹ï¼š
- âŒ "èµ„äº§ID: select_zhiyi_xxx"
- âœ… "å·²ä¸ºæ‚¨åœ¨æ·˜å®æ‰¾åˆ°10ä»¶Tæ¤ï¼Œéœ€è¦çœ‹å…·ä½“æ¬¾å¼å—ï¼Ÿ"
"""

        images = getattr(req, "images", []) or []
        image_url = getattr(req, "image_url", None)
        input_images = getattr(req, "input_images", None)
        image_ref = "ï¼ˆ[Image N] å¯¹åº”ä¸‹æ–¹é“¾æ¥ç¬¬ N å¼ ï¼‰" if images else ""
        query_references = getattr(req, "query_references", None) or []
        if query_references:
            query_references_payload = [
                ref.model_dump() if hasattr(ref, "model_dump") else ref for ref in query_references
            ]
            query_references_text = json.dumps(query_references_payload, ensure_ascii=False)
        else:
            query_references_text = "(æ— )"

        # å½“å‰è½®æ¬¡çš„ç”¨æˆ·è¯·æ±‚
        user_prompt = (
            f"### ç”¨æˆ·è¯·æ±‚\n{req.user_query}\n\n"
            f"### ç³»ç»Ÿå­—æ®µ\n"
            f"- team_id: {req.team_id}\n"
            f"- user_id: {req.user_id}\n"
            f"- session_id: {req.session_id}\n"
            f"- message_id: {req.message_id}\n\n"
            f"### ä¸šåŠ¡ä¸Šä¸‹æ–‡\n"
            f"- å¹³å°åå¥½: {getattr(req, 'preferred_entity', 'æ— ')}\n"
            f"- è¡Œä¸š: {getattr(req, 'industry', 'æ— ')}\n"
            f"- ç”¨æˆ·åå¥½: {getattr(req, 'user_preferences', 'æ— ')}\n"
            f"- abroad_type: {getattr(req, 'abroad_type', 'æ— ')}\n"
            f"- æ˜¯å¦å‚è€ƒç›‘æ§æ•°æ®: {getattr(req, 'is_monitored', 'æ— ')}\n"
            f"- æ˜¯å¦å‚è€ƒç”¨æˆ·ç”»åƒ: {getattr(req, 'is_user_preferences', 'æ— ')}\n\n"
            f"### å…³è”å®ä½“\n{query_references_text}\n\n"
            f"### è§†è§‰è¾“å…¥ {image_ref}\n{image_content or '(æ— )'}\n"
            f"- å›¾ç‰‡é“¾æ¥: {images if images else 'æ— '}\n"
            f"- image_url: {image_url or 'æ— '}\n"
            f"- input_images: {input_images or 'æ— '}\n"
        )

        # ç»„è£…æ¶ˆæ¯ï¼šSystem + History + Current
        messages = [SystemMessage(content=system_prompt)]

        # æ·»åŠ å†å²æ¶ˆæ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if history_messages:
            messages.extend(history_messages)
            logger.info(f"[æ¶ˆæ¯æ„å»º] å·²æ·»åŠ  {len(history_messages)} æ¡å†å²æ¶ˆæ¯")

        # æ·»åŠ å½“å‰è¯·æ±‚
        messages.append(HumanMessage(content=user_prompt))

        return messages

    def _should_continue(self, state: MainOrchestratorState) -> Literal["continue", "end"]:
        """åˆ¤æ–­æ˜¯å¦ç»§ç»­ ReAct å¾ªç¯ï¼ˆæ¡ä»¶è¾¹é€»è¾‘ï¼‰"""
        messages = state.get("messages") or []

        # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢æ— é™å¾ªç¯
        tool_call_turns = sum(
            1 for msg in messages if isinstance(msg, AIMessage) and getattr(msg, "tool_calls", None)
        )

        if tool_call_turns >= self._max_agent_iterations:
            logger.warning(f"[Agent] å·²è¾¾æœ€å¤§è¿­ä»£ä¸Šé™ {self._max_agent_iterations} æ¬¡")
            return "end"

        # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
        last_message = messages[-1] if messages else None
        if isinstance(last_message, AIMessage) and getattr(last_message, "tool_calls", None):
            return "continue"  # LLM å†³å®šè°ƒç”¨å·¥å…· â†’ ç»§ç»­ ReAct å¾ªç¯

        return "end"  # LLM ç»™å‡ºæœ€ç»ˆå›å¤ â†’ ç»“æŸå¾ªç¯

    # ==================== åç½®å¤„ç†èŠ‚ç‚¹ ====================

    def _postprocess_node(self, state: MainOrchestratorState) -> Dict[str, Any]:
        """åç½®å¤„ç†èŠ‚ç‚¹ï¼šæå– artifacts + æ¨é€ç»“æœ + ä¿å­˜å›å¤"""
        req = state["request"]
        messages = state.get("messages") or []

        # 1. ä» ToolMessage ä¸­æå– artifact_id å’Œå…ƒæ•°æ®ï¼ˆå®é™…æ•°æ®åœ¨ Redisï¼ŒState åªä¿å­˜å¼•ç”¨ï¼‰
        new_artifacts = {}
        for message in messages:
            if isinstance(message, ToolMessage):
                try:
                    content = json.loads(message.content)
                    if isinstance(content, dict) and content.get("status") == "success":
                        data = content.get("data")
                        if data and isinstance(data, dict) and "artifact_id" in data:
                            artifact_id = data["artifact_id"]
                            new_artifacts[artifact_id] = {
                                "type": data.get("type"),
                                "description": data.get("description"),
                                "meta": data.get("meta"),
                            }
                except Exception:
                    continue

        # 2. ä»æœ€åä¸€ä¸ªçº¯æ–‡æœ¬ AIMessage ä¸­æå–æœ€ç»ˆå›å¤
        summary_text = None
        for msg in reversed(messages):
            if isinstance(msg, AIMessage) and not getattr(msg, "tool_calls", None):
                summary_text = msg.content
                break

        if not summary_text:
            summary_text = "æŠ±æ­‰ï¼Œå¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚"

        # 2.1 æ„å»ºèµ„äº§æ‘˜è¦ï¼ˆé™„åŠ åˆ°ä¿å­˜çš„å†å²ä¸­ï¼Œæ–¹ä¾¿å¤šè½®å¯¹è¯å¼•ç”¨ï¼‰
        artifact_summary = ""
        if new_artifacts:
            artifact_lines = [
                f"- {aid}: {info.get('description', 'æ— æè¿°')}"
                for aid, info in new_artifacts.items()
            ]
            artifact_summary = "\n\nğŸ“¦ æœ¬è½®ç”Ÿæˆçš„èµ„äº§:\n" + "\n".join(artifact_lines)

        # 3. æ¨é€ç»“æœåˆ° Redis é˜Ÿåˆ—ï¼ˆä¾›å‰ç«¯å®æ—¶å±•ç¤ºï¼Œå†…éƒ¨è°ƒç”¨æ—¶è·³è¿‡ï¼‰
        if not getattr(req, "suppress_messages", False):
            finish_message = BaseRedisMessage(
                session_id=req.session_id,
                reply_message_id=req.message_id,
                reply_id=f"reply_{req.message_id}",
                reply_seq=0,
                operate_id="ç»“æœ",
                status="END",
                content_type=1,
                content=TextMessageContent(text=summary_text),
                create_ts=int(round(time.time() * 1000)),
            )
            redis_client.list_left_push(
                RedisMessageKeyName.AI_CONVERSATION_MESSAGE_QUEUE.value,
                finish_message.model_dump_json(),
            )

        # 4. å¼‚æ­¥ä¿å­˜å›å¤åˆ°æ•°æ®åº“ï¼ˆåŒ…å«èµ„äº§æ‘˜è¦ï¼Œæ–¹ä¾¿å¤šè½®å¯¹è¯å¼•ç”¨ï¼‰
        self._save_ai_response(req, summary_text + artifact_summary)

        # 5. å°è£…è¿”å›ç»“æœ
        response = WorkflowResponse(select_result=summary_text, relate_data=None)
        result = {"summary_text": summary_text, "workflow_response": response}

        # æ›´æ–° State ä¸­çš„ artifacts
        if new_artifacts:
            current_artifacts = state.get("artifacts") or {}
            current_artifacts.update(new_artifacts)
            result["artifacts"] = current_artifacts

        return result

    def _save_ai_response(self, req: WorkflowRequest, summary_text: str) -> None:
        """ä¿å­˜ AI å›å¤"""

        def insert_response() -> None:
            try:
                with pg_session(DBAlias.DB_ABROAD_AI.value) as session:
                    session.execute(
                        text(
                            """
                            INSERT INTO n8n_user_query_message(session_id, user_query)
                            VALUES (:session_id, :user_query)
                        """
                        ),
                        {"session_id": req.session_id, "user_query": f"ai:{summary_text}"},
                    )
            except Exception as e:
                logger.error(f"[ä¸»å·¥ä½œæµ] ä¿å­˜AIå›å¤å¤±è´¥: {e}")

        thread_pool.submit_with_context(insert_response)


__all__ = ["MainOrchestratorGraph"]


if __name__ == "__main__":
    """
    äº¤äº’å¼ç»ˆç«¯å¯¹è¯æ¨¡å¼

    ä½¿ç”¨æ–¹æ³•:
        python -m app.service.chains.workflow.main_orchestrator_graph

    è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º
    """
    import uuid
    import sys

    print("=" * 60)
    print("çŸ¥å°è¡£ AI åŠ©æ‰‹ - äº¤äº’å¼å¯¹è¯æ¨¡å¼")
    print("=" * 60)
    print("è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º\n")

    # åˆå§‹åŒ– graph
    graph = MainOrchestratorGraph()

    # ç”Ÿæˆä¼šè¯ IDï¼ˆæ•´ä¸ªå¯¹è¯ä¼šè¯å…±äº«ï¼‰
    session_id = f"cli_{uuid.uuid4().hex[:8]}"
    print(f"ä¼šè¯ ID: {session_id}\n")

    try:
        while True:
            # è¯»å–ç”¨æˆ·è¾“å…¥
            try:
                user_input = input("æ‚¨: ").strip()
            except (EOFError, KeyboardInterrupt):
                print("\n\nå†è§ï¼")
                sys.exit(0)

            if not user_input:
                continue

            if user_input.lower() in ("quit", "exit", "q"):
                print("\nå†è§ï¼")
                break

            # åˆ›å»ºè¯·æ±‚
            request = WorkflowRequest(
                team_id=1,
                user_id=1,
                session_id=session_id,
                message_id=f"msg_{uuid.uuid4().hex[:8]}",
                user_query=user_input,
            )

            # è¿è¡Œå·¥ä½œæµ
            print("\n[AI æ€è€ƒä¸­...]")
            try:
                state = graph.run(request)
                # æ‰“å° AI æ€è€ƒè¿‡ç¨‹ï¼ˆä» messages ä¸­æå–ï¼‰
                messages = state.get("messages") or []
                for i, msg in enumerate(messages):
                    if isinstance(msg, AIMessage):
                        content = (getattr(msg, "content", None) or "").strip()
                        if content:
                            print(f"\n[æ€è€ƒ] {content}")
                        tool_calls = getattr(msg, "tool_calls", None)
                        if tool_calls:
                            for tc in tool_calls:
                                name = tc.get("name", "?")
                                args = tc.get("args") or {}
                                print(f"[è°ƒç”¨å·¥å…·] {name} å‚æ•°: {args}")
                    elif isinstance(msg, ToolMessage):
                        try:
                            content = (
                                json.loads(msg.content)
                                if isinstance(msg.content, str)
                                else msg.content
                            )
                            status = content.get("status", "?")
                            brief = content.get("message", str(content)[:80])
                            print(f"[å·¥å…·ç»“æœ] {msg.name} -> {status}: {brief}")
                        except Exception:
                            print(f"[å·¥å…·ç»“æœ] {msg.name} -> (è§ä¸Šæ–¹æ—¥å¿—)")

                # æå–å›å¤
                summary_text = state.get("summary_text")
                if not summary_text:
                    # ä» messages ä¸­æå–æœ€åä¸€æ¡ AI æ¶ˆæ¯
                    messages = state.get("messages") or []
                    for msg in reversed(messages):
                        if isinstance(msg, AIMessage) and not getattr(msg, "tool_calls", None):
                            summary_text = msg.content
                            break

                if summary_text:
                    print(f"\nçŸ¥å°è¡£: {summary_text}\n")
                else:
                    print("\nçŸ¥å°è¡£: æŠ±æ­‰ï¼Œæœªèƒ½ç”Ÿæˆå›å¤ã€‚\n")

            except Exception as e:
                logger.exception(f"è¿è¡Œå·¥ä½œæµæ—¶å‡ºé”™: {e}")
                print(f"\n[é”™è¯¯] {e}\n")

    except KeyboardInterrupt:
        print("\n\nå†è§ï¼")
        sys.exit(0)
