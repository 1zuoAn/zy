import json
import os
import asyncio
from typing import Optional, List, Dict, Any, AsyncGenerator
from uuid import uuid4
from datetime import datetime
from contextlib import asynccontextmanager

from pydantic import ValidationError
from openai import AsyncOpenAI

# ReMe & FlowLLM SDK
from reme_ai import ReMeApp
from flowllm.core.embedding_model.openai_compatible_embedding_model import OpenAICompatibleEmbeddingModel
from flowllm.core.vector_store.es_vector_store import EsVectorStore
from flowllm.core.schema import VectorNode

# å¼•å…¥åŒçº§é…ç½®å’ŒSchema
from . import config, prompt_template
from .patches import apply_monkey_patches
from app.schemas.request.intent_request import (
    ClassifyRequest, FeedbackRequest, MemoryMaintainRequest, IntentEnum
)
from app.schemas.response.intent_response import (
    ClassifyResult, ClassifyResponse, CLASSIFY_JSON_SCHEMA
)
from ...config import settings
from ...core.config.constants import LlmModelName, EmbeddingModelName

# 1. åº”ç”¨ Monkey Patches (æœ€ä¼˜å…ˆæ‰§è¡Œ)
apply_monkey_patches()

class IntentService:
    def __init__(self):
        # å…¨å±€å•ä¾‹çŠ¶æ€
        self.reme_app: Optional[ReMeApp] = None
        self.openai_client: Optional[AsyncOpenAI] = None
        self.maintenance_vs: Optional[EsVectorStore] = None
        self._is_ready = False

        # æ¨¡å‹å›é€€é“¾é…ç½®
        self.MODEL_FALLBACK_CHAIN = [
            LlmModelName.OPENROUTER_GEMINI_3_FLASH_PREVIEW.value,
            LlmModelName.OPENROUTER_GEMINI_2_5_FLASH.value,
            LlmModelName.OPENROUTER_GPT_4O.value
        ]

    # ===========================================================================
    # 1. ç”Ÿå‘½å‘¨æœŸç®¡ç† (Startup / Shutdown)
    # ===========================================================================
    
    async def startup(self):
        """å¯¹åº”åŸ main.py çš„ lifespan startup éƒ¨åˆ†"""
        print("ğŸš€ [IntentService] Initializing...")

        # --- A. åˆå§‹åŒ– OpenAI Client ---
        self.openai_client = AsyncOpenAI(
            base_url=settings.openrouter_api_base,
            api_key=settings.openrouter_api_key,
        )

        # --- B. åˆå§‹åŒ–ç»´æŠ¤ç”¨å‘é‡åº“ (Maintenance VS) ---
        print("ğŸ”§ [IntentService] Initializing Standalone Maintenance Vector Store...")
        
        # ç‹¬ç«‹åˆå§‹åŒ– Embedding æ¨¡å‹
        maintenance_embedding = OpenAICompatibleEmbeddingModel(
            model_name=EmbeddingModelName.DASHSCOPE_TEXT_EMBEDDING_V4.value,
            api_key=settings.dashscope_api_key,
            base_url=settings.dashscope_api_base,
        )

        # ç‹¬ç«‹åˆå§‹åŒ– ES è¿æ¥
        try:
            # ç®€å•å¤„ç† ES_HOST (ç§»é™¤ http:// å‰ç¼€å¦‚æœå­˜åœ¨ï¼Œå› ä¸º EsVectorStore å¯èƒ½è‡ªåŠ¨åŠ )
            # è¿™é‡ŒæŒ‰ç…§ä½ åŸå§‹ä»£ç é€»è¾‘ä¿æŒä¸€è‡´
            hosts = [f"http://{settings.es_host}"]
            
            self.maintenance_vs = EsVectorStore(
                hosts=hosts,
                basic_auth=(settings.es_user, settings.es_password),
                embedding_model=maintenance_embedding
            )
            print("âœ… [IntentService] Maintenance Connection Ready.")
        except Exception as e:
            print(f"âš ï¸ [IntentService] Maintenance VS Init Failed: {e}")

        # --- C. åˆå§‹åŒ– ReMe App ---
        print(f"ğŸš€ [IntentService] Initializing ReMe (Backend: Elasticsearch)...")

        # æ³¨å…¥ ReMe æ‰€éœ€çš„ç¯å¢ƒå˜é‡
        if settings.openrouter_api_key:
            os.environ["FLOW_LLM_API_KEY"] = settings.openrouter_api_key
        os.environ["FLOW_LLM_BASE_URL"] = settings.openrouter_api_base

        if settings.dashscope_api_key:
            os.environ["FLOW_EMBEDDING_API_KEY"] = settings.dashscope_api_key
        else:
            print("âš ï¸ è­¦å‘Š: æœªæ£€æµ‹åˆ° DASHSCOPE_API_KEY")
            os.environ["FLOW_EMBEDDING_API_KEY"] = "dummy"

        os.environ["FLOW_EMBEDDING_BASE_URL"] = settings.dashscope_api_base

        # æ„é€  ES å‚æ•°
        es_url = f"http://{settings.es_user}:{settings.es_password}@{settings.es_host}" if settings.es_user else None
        es_params_json = json.dumps({"hosts": es_url})

        self.reme_app = ReMeApp(
            f"llm.default.api_key={settings.openrouter_api_key}",
            f"llm.default.base_url={settings.openrouter_api_base}",
            f"llm.default.model_name={LlmModelName.OPENROUTER_GEMINI_3_FLASH_PREVIEW.value}",
            "llm.default.backend=openai_compatible",
            f"embedding_model.default.model_name={EmbeddingModelName.DASHSCOPE_TEXT_EMBEDDING_V4.value}",
            "embedding_model.default.backend=openai_compatible",
            "vector_store.default.backend=elasticsearch",
            f"vector_store.default.params={es_params_json}",
            "init_logger=false",  # ç¦ç”¨ flowllm çš„æ—¥å¿—åˆå§‹åŒ–ï¼Œä¿ç•™åº”ç”¨è‡ªå·±çš„æ—¥å¿—é…ç½®
        )
        
        await self.reme_app.async_start()
        self._is_ready = True
        print("âœ… [IntentService] Service Fully Started.")

    async def shutdown(self):
        """å¯¹åº”åŸ main.py çš„ lifespan shutdown éƒ¨åˆ†"""
        if self.reme_app:
            await self.reme_app.async_stop()
        if self.maintenance_vs:
            await self.maintenance_vs.async_close()
            print("ğŸ›‘ [IntentService] Maintenance Connection Closed")
        self._is_ready = False
        print("ğŸ›‘ [IntentService] Stopped")

    # ===========================================================================
    # 2. å†…éƒ¨ç§æœ‰è¾…åŠ©æ–¹æ³• (åŸ main.py ä¸­çš„ç‹¬ç«‹å‡½æ•°)
    # ===========================================================================

    def _construct_standard_node(
        self,
        workspace_id: str,
        unique_id: str,
        when_to_use: str,   # è§¦å‘æ¡ä»¶
        answer: str,        # å®é™…å›ç­”
        tags: List[str],
        author: str = "manual"
    ) -> VectorNode:
        """
        æ„é€ ç¬¦åˆ ReMe/ES æ ‡å‡†çš„ VectorNode ç»“æ„ (åŸæ ·ä¿ç•™)
        """
        now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # 1. æ„é€ å†…å±‚ Metadata
        inner_meta_dict = {
            "when_to_use": when_to_use,
            "experience": answer,
            "tags": tags,
            "confidence": 1.0,
            "step_type": "decision",
            "tools_used": []
        }
        
        # 2. æ„é€ å¤–å±‚ Metadata
        outer_meta = {
            "memory_type": "task",
            "content": answer,
            "score": 1.0,
            "time_created": now_str,
            "time_modified": now_str,
            "author": author,
            "metadata": json.dumps(inner_meta_dict, ensure_ascii=False) 
        }

        return VectorNode(
            unique_id=unique_id,
            workspace_id=workspace_id,
            content=when_to_use,    # Trigger ç”¨äº Embedding
            metadata=outer_meta,
            vector=None             # ç¨åè®¡ç®—
        )

    async def _batch_insert(self, nodes: List[VectorNode]):
        """æ‰¹é‡æ’å…¥è¾…åŠ©å‡½æ•° (åŸæ ·ä¿ç•™)"""
        if not self.maintenance_vs: return
        try:
            texts = [n.content for n in nodes]
            # ä½¿ç”¨ maintenance_vs è‡ªå¸¦çš„ embedding model
            embeddings = self.maintenance_vs.embedding_model.get_embeddings(texts)
            if not embeddings: 
                raise ValueError("Embeddings generation returned empty")
            
            for i, node in enumerate(nodes):
                node.vector = embeddings[i] 
                
            await self.maintenance_vs.async_insert(nodes, workspace_id=nodes[0].workspace_id)
            print(f"   âœ… Inserted batch of {len(nodes)}")
        except Exception as e:
            print(f"   âš ï¸ Batch insert failed: {e}")
            raise e

    async def _llm_call_with_retry(self, messages: List[dict]) -> ClassifyResult:
        """å¸¦é‡è¯•æœºåˆ¶çš„ LLM è°ƒç”¨ (åŸæ ·ä¿ç•™é€»è¾‘)"""
        last_exception = None
        unique_models = []
        seen = set()
        
        # æ„å»ºå»é‡åçš„æ¨¡å‹åˆ—è¡¨
        for m in self.MODEL_FALLBACK_CHAIN:
            if m and m not in seen:
                unique_models.append(m)
                seen.add(m)

        for model_name in unique_models:
            print(f"ğŸ¤– [IntentService] Trying model: {model_name}...")
            try:
                if not self.openai_client: raise ValueError("OpenAI Client not initialized")
                
                completion = await self.openai_client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    timeout=30,
                    temperature=0,
                    extra_body={
                        "response_format": {
                            "type": "json_schema",
                            "json_schema": CLASSIFY_JSON_SCHEMA
                        }
                    }
                )
                usage_stats = {
                    "input_tokens": completion.usage.prompt_tokens if completion.usage else 0,
                    "output_tokens": completion.usage.completion_tokens if completion.usage else 0
                }
                if not completion.choices or not completion.choices[0].message.content:
                    raise ValueError("Empty response from LLM")
                content = completion.choices[0].message.content.strip()
                
                # æ¸…æ´— markdown
                if content.startswith("```"):
                    content = content.replace("```json", "").replace("```", "").strip()
                
                try:
                    result = ClassifyResult.model_validate_json(content)
                except ValidationError:
                    try:
                        temp = json.loads(content)
                        if isinstance(temp, str):
                            result = ClassifyResult.model_validate_json(temp)
                        else:
                            result = ClassifyResult.model_validate(temp)
                    except Exception:
                        raise ValueError(f"Failed to parse JSON: {content[:100]}...")
                return result, usage_stats
            except Exception as e:
                print(f"âš ï¸ Model {model_name} failed: {e}")
                last_exception = e
                continue
        
        raise Exception(f"All models failed. Last error: {last_exception}")

    # ===========================================================================
    # 3. æ ¸å¿ƒä¸šåŠ¡æ–¹æ³• (å¯¹åº”åŸ API æ¥å£é€»è¾‘)
    # ===========================================================================

    async def predict_intent(self, req: ClassifyRequest) -> ClassifyResponse:
        """å¯¹åº”åŸ /classify æ¥å£"""
        if not self._is_ready:
            raise RuntimeError("IntentService not initialized. Check lifespan.")

        def _extract_user_query_text(raw_query: str) -> str:
            """ä»…ç”¨äºåˆ¤æ–­ç”¨æˆ·æ˜¯å¦åœ¨ query æ˜ç¡®æåˆ°å…³é”®å­—ï¼Œæ’é™¤ã€Œæ•°æ®æºã€æ‹¼æ¥å†…å®¹ã€‚"""
            if not raw_query:
                return ""
            # ä»…ä¿ç•™â€œæ•°æ®æºï¼šâ€ä¹‹å‰çš„å†…å®¹ï¼Œé¿å…å‰ç«¯å‹¾é€‰é¡¹å½±å“åˆ¤æ–­
            return raw_query.split("æ•°æ®æº", 1)[0].strip()

        memory_context = ""
        search_query = req.query
        if req.history:
            search_query = req.history + req.query

        # 1. æ£€ç´¢ ReMe
        try:
            res = await self.reme_app.async_execute(
                name="retrieve_task_memory_simple",
                workspace_id=config.UNIFIED_WORKSPACE_ID,
                query=search_query,
                top_k=5
            )
            if isinstance(res, dict) and "answer" in res:
                memory_context = res["answer"]
            elif hasattr(res, "result"):
                memory_context = str(res.result)
            if memory_context is None: memory_context = ""
            
            print(f"ğŸ§  Retrieved Context ({len(memory_context)} chars): {memory_context[:50]}...")
        except Exception as e:
            print(f"âš ï¸ Memory Retrieve Skipped: {e}")

        # 2. æ„é€  Prompt
        display_context = memory_context if memory_context else "æš‚æ— ç›¸å…³å†å²ç»éªŒ"
        full_prompt = prompt_template.N8N_SYSTEM_PROMPT.format(memory_context=display_context)
        formatted_history = req.history if req.history else "No History"

        user_input = f"""
        <user_context>
            <preferred_entity_selection>
                {req.preferred_entity or "None (User did not select)"}
            </preferred_entity_selection>
            <conversation_history>
                {formatted_history}
            </conversation_history>
        </user_context>
        <current_query>
            {req.query}
        </current_query>
        <instruction>
            Please classify the intent of the content in <current_query>.
            Note: If the intent in <current_query> conflicts with <preferred_entity_selection>, trust the explicit intent in <current_query>.
        </instruction>
        """
        
        messages = [
            {"role": "system", "content": full_prompt},
            {"role": "user", "content": user_input}
        ]

        # 3. è°ƒç”¨ LLM
        try:
            result, token_stats = await self._llm_call_with_retry(messages)
            user_query_text = _extract_user_query_text(req.query).lower()
            if "ins" in user_query_text and result.category == IntentEnum.SELECTION:
                print(f"ğŸ”„ [IntentService] Auto-correct: 'ins' detected. SELECTION -> MEDIA.")
                result.category = IntentEnum.MEDIA
                result.reasoning = f"(è‡ªåŠ¨çº æ­£) æ£€æµ‹åˆ°å…³é”®å­— 'ins'ï¼Œå¼ºåˆ¶ä»é€‰å“çº æ­£ä¸ºåª’ä½“ã€‚åŸå› ä¸º: {result.reasoning}"
            # ç»„è£… Response
            return ClassifyResponse(
                category=result.category,
                reasoning=result.reasoning,
                memory_used=bool(memory_context),
                retrieved_context=display_context,
                input_tokens=token_stats["input_tokens"],   
                output_tokens=token_stats["output_tokens"]  
            )
        except Exception as e:
            print(f"ğŸ”¥ ALL RETRIES FAILED: {e}")
            # å…œåº•è¿”å›
            return ClassifyResponse(
                category=IntentEnum.CHATBOT,
                reasoning=f"System Fallback: {str(e)}",
                memory_used=False,
                retrieved_context="Error"
            )

    async def process_feedback(self, req: FeedbackRequest):
        """å¯¹åº”åŸ /feedback æ¥å£çš„åå°ä»»åŠ¡é€»è¾‘"""
        if not self._is_ready: return
        try:
            print(f"ğŸ§  Learning: {req.query} -> {req.correct_category.value}")
            await self.reme_app.async_execute(
                name="summary_task_memory",
                workspace_id=config.UNIFIED_WORKSPACE_ID,
                trajectories=[{
                    "messages": [
                        {"role": "user", "content": req.query},
                        {"role": "assistant", "content": f"Category: {req.correct_category.value}\nReason: {req.reason}"}
                    ],
                    "score": 1.0
                }]
            )
            print("âœ… Memory Saved.")
        except Exception as e:
            print(f"âŒ Learning Failed: {e}")

    # ===========================================================================
    # 4. ç»´æŠ¤ç›¸å…³æ–¹æ³• (å¯¹åº”åŸ Maintenance æ¥å£é€»è¾‘)
    # ===========================================================================

    async def import_memories_from_text(self, lines: List[str], workspace_id_override: Optional[str] = None) -> int:
        """
        å¯¹åº”åŸ /maintenance/import_jsonl æ¥å£é€»è¾‘
        æ³¨æ„ï¼šè¿™é‡Œæ¥æ”¶çš„æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ (lines)ï¼Œæ–‡ä»¶è¯»å–æ­¥éª¤æ”¾åœ¨ Endpoint å±‚å¤„ç†
        """
        if not self.maintenance_vs:
            raise RuntimeError("Maintenance Service not initialized")

        batch_nodes = []
        processed = 0
        
        for line in lines:
            if not line.strip(): continue
            try:
                raw = json.loads(line)
                uid = raw.get("unique_id", uuid4().hex)
                wid = workspace_id_override or raw.get("workspace_id", config.UNIFIED_WORKSPACE_ID)
                
                # --- æ™ºèƒ½åˆ¤æ–­é€»è¾‘ (åŸæ ·ä¿ç•™) ---
                raw_meta = raw.get("metadata", {})
                
                # æƒ…å†µ A: æ ‡å‡†å¤æ‚æ ¼å¼
                if "metadata" in raw_meta and isinstance(raw_meta["metadata"], str):
                     node = VectorNode(
                        unique_id=uid,
                        workspace_id=wid,
                        content=raw.get("content"), 
                        metadata=raw_meta,
                        vector=None
                    )
                
                # æƒ…å†µ B: ç®€å•æ ¼å¼ -> å‡çº§ä¸ºæ ‡å‡†æ ¼å¼
                else:
                    answer = raw_meta.get("content") or raw.get("answer") or "No Content"
                    tags = raw_meta.get("tags", [])
                    
                    node = self._construct_standard_node(
                        workspace_id=wid,
                        unique_id=uid,
                        when_to_use=raw.get("content"), 
                        answer=answer,
                        tags=tags,
                        author="batch_import"
                    )
                
                batch_nodes.append(node)
                
                if len(batch_nodes) >= 10: 
                    await self._batch_insert(batch_nodes)
                    processed += len(batch_nodes)
                    batch_nodes = []
                    
            except Exception as e:
                print(f"âŒ Error line: {e}")

        if batch_nodes:
            await self._batch_insert(batch_nodes)
            processed += len(batch_nodes)
        
        return processed

    async def upsert_memory(self, req: MemoryMaintainRequest) -> str:
        """å¯¹åº”åŸ /maintenance/memory æ¥å£"""
        if not self.maintenance_vs: 
            raise RuntimeError("Maintenance Service not initialized")
        
        try:
            final_uid = req.unique_id if req.unique_id else uuid4().hex

            # ä½¿ç”¨ _construct_standard_node ç¡®ä¿ç»“æ„ä¸€è‡´
            node = self._construct_standard_node(
                workspace_id=req.workspace_id,
                unique_id=final_uid,
                when_to_use=req.when_to_use,
                answer=req.content,
                tags=req.tags,
                author="api_manual"
            )
            
            # ç”Ÿæˆå‘é‡
            emb = self.maintenance_vs.embedding_model.get_embeddings([node.content])
            if not emb: 
                raise Exception("Embedding failed")
            node.vector = emb[0]
            
            # æ’å…¥
            await self.maintenance_vs.async_insert([node], workspace_id=req.workspace_id)
            print(f"âœ… Manual Memory Saved (Standardized): {final_uid}")
            return final_uid
            
        except Exception as e:
            print(f"âŒ Upsert Error: {e}")
            raise e

    async def list_memories(self, workspace_id: str, limit: int) -> Dict[str, Any]:
        """å¯¹åº”åŸ /maintenance/list æ¥å£"""
        if not self.maintenance_vs:
            raise RuntimeError("Maintenance Service not initialized")
        
        nodes = await self.maintenance_vs.async_list_workspace_nodes(workspace_id=workspace_id, max_size=limit)
        
        result = []
        for node in nodes:
            node_dict = node.model_dump()
            node_dict.pop("vector", None) # éšè—å‘é‡
            result.append(node_dict)
            
        return {
            "workspace_id": workspace_id,
            "total_retrieved": len(result),
            "items": result
        }

    async def clear_workspace(self, workspace_id: str) -> Dict[str, str]:
        """å¯¹åº”åŸ /maintenance/clear æ¥å£"""
        if not self.maintenance_vs:
             raise RuntimeError("Maintenance Service not initialized")
        
        exists = await self.maintenance_vs.async_exist_workspace(workspace_id)
        if not exists:
            return {"status": "skipped", "message": f"Workspace {workspace_id} does not exist."}
            
        await self.maintenance_vs.async_delete_workspace(workspace_id)
        print(f"ğŸ”¥ Workspace {workspace_id} deleted.")
        
        await self.maintenance_vs.async_create_workspace(workspace_id)
        print(f"âœ… Workspace {workspace_id} recreated.")
        
        return {"status": "success", "message": f"All memories in {workspace_id} have been cleared."}

    async def export_memories_generator(self, workspace_id: str) -> AsyncGenerator[str, None]:
        """
        å¯¹åº”åŸ /maintenance/export æ¥å£çš„æ ¸å¿ƒé€»è¾‘
        è¿”å›ä¸€ä¸ª AsyncGeneratorï¼Œä¾› Endpoint å°è£…ä¸º StreamingResponse
        """
        if not self.maintenance_vs:
            raise RuntimeError("Maintenance Service not initialized")

        # 1. æ‹‰å–æ•°æ® (ä¿ç•™åŸä»£ç é€»è¾‘ max_size=10000)
        nodes = await self.maintenance_vs.async_list_workspace_nodes(workspace_id=workspace_id, max_size=10000)
        print(f"ğŸ“¤ Exporting {len(nodes)} nodes from {workspace_id}...")

        # 2. ç”Ÿæˆå™¨é€»è¾‘
        for node in nodes:
            node_dict = node.model_dump()
            node_dict["vector"] = [] # æ¸…ç©ºå‘é‡
            yield json.dumps(node_dict, ensure_ascii=False) + "\n"

# å®ä¾‹åŒ–å•ä¾‹
intent_service = IntentService()